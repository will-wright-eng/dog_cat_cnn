# -*- coding: utf-8 -*-
"""cats_vs_dogs_UCBfinal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16cP31S6Gsc7ziVc_gsQ2cQME6TEUMCBE

## Image Classification Model - Cats🐱 vs Dogs🐶 
By Kadar Amek, Joseph Filla, Will Wright.


"""

# #First, let's download the "Dogs vs. Cats" dataset from Kaggle, which contains 25,000 images and use a subset of the full dataset to decrease training time.

## IMPORTED DATASET
# extract.zip file contains train and validation data

!wget --no-check-certificate \
    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \
    -O /home/cats_and_dogs_filtered.zip

####
#### Break in code
####


## UNZIP IMPORTED DATASET
# only needs to be run once

import os
import zipfile

local_zip = '/home/cats_and_dogs_filtered.zip' 
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/home')
zip_ref.close()

####
#### Break in code
####

#Keras is an Open Source Neural Network library 
#written in Python that runs on top of Theano or Tensorflow
import keras
#image augmentation artifically expand the size of training dataset by creating
#motivate version of images
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Model
from keras.layers import Input, Flatten, Dense, Dropout, GlobalAveragePooling2D
from keras.applications.mobilenet import MobileNet
import math

import matplotlib. pyplot as plt
import numpy as np
import pandas as pd
from time import process_time 

# input parameters and directories
TRAIN_DATA_DIR = '/home/cats_and_dogs_filtered/train/'
VALIDATION_DATA_DIR = '/home/cats_and_dogs_filtered/validation/'
# TRAIN_SAMPLES = 2000
TRAIN_SAMPLES = 500
# VALIDATION_SAMPLES = 100
VALIDATION_SAMPLES = 25
NUM_CLASSES=2
IMG_WIDTH, IMG_HEIGHT = 224, 224
# BATCH_SIZE=64
BATCH_SIZE=5
# VALIDATION_BATCH_SIZE = 64
VALIDATION_BATCH_SIZE = 5

## DATA PRE-PROCESSING
# ImageDataGenerator class: 
    # Generate batches of tensor image data with real-time data augmentation. 
    # The data will be looped over (in batches).

train_datagen = ImageDataGenerator(rescale=1./255,
                                  rotation_range=20,
                                  width_shift_range=0.2,
                                  height_shift_range=0.2,
                                  zoom_range=0.2)

train_generator = train_datagen.flow_from_directory(
                        TRAIN_DATA_DIR,
                        target_size=(IMG_WIDTH, IMG_HEIGHT),
                        batch_size=BATCH_SIZE,
                        shuffle=True,
                        seed=12345,
                        class_mode='categorical')

val_datagen = ImageDataGenerator(rescale=1./255)
validation_generator = val_datagen.flow_from_directory(
                        VALIDATION_DATA_DIR,
                        target_size=(IMG_WIDTH, IMG_HEIGHT),
                        batch_size=BATCH_SIZE,
                        shuffle=False,
                        class_mode='categorical')

def model_iterations(var_one,var_two):
  loop_string = '_dense'+str(var_one)+'_drop'+str(var_two)
  print('\n\n',loop_string,'\n\n')

  ## BUILD MODEL WITH MOBILENET BASE

  def model_maker():
      base_model = MobileNet(include_top=False, input_shape = (IMG_WIDTH,IMG_HEIGHT,3))
      for layer in base_model.layers[:]:
          layer.trainable = False # Freeze the layers
      input = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))
      custom_model = base_model(input)
      custom_model = GlobalAveragePooling2D()(custom_model)
      # iterate over var_one in for loop
      custom_model = Dense(var_one, activation='relu')(custom_model)
      # iterate over var_two in for loop
      custom_model = Dropout(var_two)(custom_model)
      predictions = Dense(NUM_CLASSES, activation='softmax')(custom_model)
      return Model(inputs=input, outputs=predictions)

  model = model_maker()
  model.compile(loss='categorical_crossentropy',
                optimizer= keras.optimizers.Adam(lr=0.001),
                metrics=['acc'])
  history = model.fit_generator(train_generator,
                      steps_per_epoch = math.ceil(float(TRAIN_SAMPLES) / BATCH_SIZE),
                      epochs=10,
                      validation_data = validation_generator,
                      validation_steps = math.ceil(float(VALIDATION_SAMPLES) / BATCH_SIZE))

  model.save('model'+loop_string+'.h5')

  ## VISUALIZE MODEL FIT PERFORMANCE
  print(history)
  #import matplotlib. pyplot as plt
  # Retrieve a list of accuracy results on training and validation data
  # sets for each training epoch
  acc = history.history['acc']
  val_acc = history.history['val_acc']
  # Retrieve a list of list results on training and validation data
  # sets for each training epoch
  loss = history.history['loss']
  val_loss = history.history['val_loss']
  # Get number of epochs
  epochs = range(len(acc))
  # Plot training and validation accuracy per epoch
  plt.plot(epochs, acc)
  plt.plot(epochs, val_acc)
  plt.title('Training (blue) and validation (orange) accuracy')
  plt.savefig('accuracy'+loop_string+'.png',dpi=300)
  plt.show()

  plt.figure()
  # Plot training and validation loss per epoch
  plt.plot(epochs, loss)
  plt.plot(epochs, val_loss)
  plt.title('Training (blue) and validation (orange) loss')
  plt.savefig('loss'+loop_string+'.png',dpi=300)
  plt.show()

  ## VALIDATE MODEL AND PARSE PREDICTIONS
  # model.predict_generator

  validation_datagen = ImageDataGenerator(rescale=1./255)
  validation_generator = validation_datagen.flow_from_directory(
          VALIDATION_DATA_DIR,
          target_size=(IMG_WIDTH, IMG_HEIGHT),
          batch_size=VALIDATION_BATCH_SIZE,
          shuffle=False,
          class_mode='categorical')

  ground_truth = validation_generator.classes
  predictions = model.predict_generator(validation_generator, 
      steps=(1000/VALIDATION_BATCH_SIZE))

  # prediction_table is a dict with index, prediction, ground truth
  prediction_table = {}
  for index, val in enumerate(predictions):
      #get argmax index
      index_of_highest_probability = np.argmax(val)
      value_of_highest_probability = val[index_of_highest_probability]
      prediction_table[index] = [value_of_highest_probability, index_of_highest_probability,
          ground_truth[index]]
  assert len(predictions) == len(ground_truth) == len(prediction_table)

  df = pd.DataFrame(prediction_table).T
  df.columns = ['probability','cat0_dog1','ground_truth']
  df.to_csv('predictions'+loop_string+'.csv')
  return

####
#### Break in code
####

## ITERATE OVER DENSE & DROPOUT LAYERS
# NOTE: Change Notebook Settings / Hardware accelerator to 'GPU'
# with current settings each loop takes approximately 90 seconds

m = np.arange(12, 64, 12).tolist() # var_one
l = np.arange(0.1, 1, 0.2).tolist() # var_two
l = [round(i, 2) for i in l]

# assert len(l)==len(m)

# define function here with two inputs for dense layer size (var_one) and dropout (var_two)
# var_one = 12 --> number of nodes in dense layer
# var_two = 0.1 --> percent dropout 

entire_process_start = process_time() 

for i in m:
  for j in l:
    # Start the stopwatch / counter 
    t1_start = process_time() 
    # run model
    # model_iterations(var_one,var_two):
    model_iterations(i,j)
    # Stop the stopwatch / counter 
    t1_stop = process_time() 
    print('Elapsed time during loop:', t1_stop-t1_start, ' [seconds]') 

entire_process_stop = process_time() 
print('Elapsed time during all loops:', entire_process_stop-entire_process_start, ' [seconds]') 

####
#### Break in code
####

## DOWNLOAD CONTENT FROM PROGRAM

# !zip -r /content/file.zip /content

####
#### Break in code
####

from google.colab import files
files.download("/content/file.zip")
